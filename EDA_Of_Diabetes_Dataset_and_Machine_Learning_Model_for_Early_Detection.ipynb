{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umarzaib1/Early-Detection-of-Diabetes-from-Its-symptoms-using-Machine-Learning-/blob/main/EDA_Of_Diabetes_Dataset_and_Machine_Learning_Model_for_Early_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf2c4eee"
      },
      "source": [
        "## Project Details\n",
        "\n",
        "**Project Title:** EDA of Diabetes and Early detection of diabetes from its symptoms Using Machine Learning\n",
        "\n",
        "**Project Goal:** To perform data analysis on the provided dataset and develop machine learning models for early detection of diabetes from its symptoms. The final goal is to identify a suitable model for potential deployment in an Android application for general users.\n",
        "\n",
        "**Role:** Student of last semester of bachelors, aiming to showcase skills in data analysis, machine learning model development, and evaluation.\n",
        "\n",
        "**Step-by-Step Process:**\n",
        "\n",
        "1.  **Project Setup and Data Loading:**\n",
        "    *   Set up the Colab environment, import necessary libraries, and load the `diabetes.csv` dataset.\n",
        "    *   Add initial markdown cells to introduce the project and the dataset.\n",
        "\n",
        "2.  **Exploratory Data Analysis (EDA):**\n",
        "    *   Perform a thorough EDA to understand the dataset's structure, features, and target variable.\n",
        "    *   Analyze descriptive statistics (mean, median, standard deviation, etc.).\n",
        "    *   Visualize the distribution of individual features (histograms, box plots).\n",
        "    *   Explore the relationships between features and the target variable (scatter plots, bar plots, correlation matrices).\n",
        "    *   Identify missing values, outliers, and potential data inconsistencies.\n",
        "    *   Document findings and insights using markdown cells.\n",
        "\n",
        "3.  **Data Preprocessing and Feature Engineering:**\n",
        "    *   Handle missing values (e.g., imputation).\n",
        "    *   Address outliers if necessary.\n",
        "    *   Encode categorical features (if any).\n",
        "    *   Scale or normalize numerical features if required by the chosen models.\n",
        "    *   Consider creating new features that might improve model performance.\n",
        "    *   Split the data into training and testing sets.\n",
        "\n",
        "4.  **Model Selection and Training:**\n",
        "    *   Select appropriate machine learning models for binary classification (e.g., Logistic Regression, Support Vector Machines, Tree-based models like Random Forest and Gradient Boosting).\n",
        "    *   Justify the choice of models based on the data characteristics and project goals.\n",
        "    *   Train the selected models on the training data.\n",
        "\n",
        "5.  **Model Evaluation:**\n",
        "    *   Evaluate the performance of the trained models using relevant metrics (e.g., Accuracy, Precision, Recall, F1-score, ROC AUC).\n",
        "    *   Use techniques like cross-validation for more robust evaluation.\n",
        "    *   Analyze confusion matrices to understand model performance in terms of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "6.  **Hyperparameter Tuning:**\n",
        "    *   Tune the hyperparameters of the selected models to optimize their performance.\n",
        "    *   Use techniques like GridSearchCV or RandomizedSearchCV.\n",
        "\n",
        "7.  **Final Model Selection and Interpretation:**\n",
        "    *   Select the best-performing model based on the evaluation metrics and the project's goals (considering the impact of false positives and false negatives for a medical application).\n",
        "    *   Interpret the chosen model to understand which features are most important for prediction.\n",
        "\n",
        "8.  **Model Export and Preparation for Deployment:**\n",
        "    *   Save the trained and tuned final model (e.g., using `joblib` or `pickle`).\n",
        "    *   Document the model's requirements (input features, data types, scaling/preprocessing steps needed before prediction).\n",
        "\n",
        "9.  **Documentation and Presentation:**\n",
        "    *   Organize the notebook logically with clear headings and explanations in markdown cells.\n",
        "    *   Summarize the EDA findings, model performance, and conclusions.\n",
        "    *   Prepare a presentation summarizing the project, methodology, results, and the chosen model's suitability for the Android application.\n",
        "\n",
        "10. **Considerations for Android Application:**\n",
        "    *   Discuss the technical requirements for integrating the model into an Android app (e.g., using TensorFlow Lite, ONNX Runtime, or a cloud-based API).\n",
        "    *   Mention potential challenges and future work related to deployment.\n",
        "\n",
        "11. **Finish task:** Review the entire project, ensure all requirements are met, and prepare for final submission and presentation."
      ],
      "id": "cf2c4eee"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36935dda"
      },
      "source": [
        "### Step 1: Project Setup and Data Loading\n",
        "\n",
        "This step involves importing the necessary libraries and loading the dataset into a pandas DataFrame for further analysis."
      ],
      "id": "36935dda"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fe047fe"
      },
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import joblib"
      ],
      "id": "6fe047fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset"
      ],
      "metadata": {
        "id": "p981XnwVCOT1"
      },
      "id": "p981XnwVCOT1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Display the first few rows and the dataset information\n",
        "display(data.head())\n",
        "display(data.info())"
      ],
      "metadata": {
        "id": "GmjgSSD9CKQ_"
      },
      "id": "GmjgSSD9CKQ_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9f67400"
      },
      "source": [
        "### Step 2: Exploratory Data Analysis (EDA)\n",
        "\n",
        "In this step, we will explore the dataset to understand its structure, features, and the distribution of the target variable. We will also check for missing values and outliers."
      ],
      "id": "f9f67400"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9db2bcee"
      },
      "source": [
        "# Display descriptive statistics\n",
        "display(data.describe())\n",
        "\n",
        "# Check for missing values\n",
        "display(data.isnull().sum())\n",
        "\n",
        "# Display the distribution of the target variable\n",
        "display(data['class'].value_counts())\n",
        "\n",
        "# Visualize the distribution of the target variable\n",
        "sns.countplot(x='class', data=data)\n",
        "plt.title('Distribution of Diabetes (0: No, 1: Yes)')\n",
        "plt.show()"
      ],
      "id": "9db2bcee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d29c5765"
      },
      "source": [
        "### Step 3: Data Preprocessing and Feature Engineering\n",
        "\n",
        "In this step, we will handle missing values, address potential outliers, encode categorical features (if any), scale numerical features, and split the data into training and testing sets."
      ],
      "id": "d29c5765"
    },
    {
      "cell_type": "code",
      "source": [
        "  # data preprocessing of every feature in data\n",
        "enc=preprocessing.LabelEncoder()\n",
        "age=data[\"Age\"]\n",
        "polyuria=list(enc.fit_transform(data[\"Polyuria\"]))\n",
        "polydipsia=list(enc.fit_transform(data[\"Polydipsia\"]))\n",
        "sdn_wt_ls=list(enc.fit_transform(data[\"sudden weight loss\"]))\n",
        "weakness=list(enc.fit_transform(data[\"weakness\"]))\n",
        "polyphagia=list(enc.fit_transform(data[\"Polyphagia\"]))\n",
        "gntl_trsh=list(enc.fit_transform(data[\"Genital thrush\"]))\n",
        "visual_blur=list(enc.fit_transform(data[\"visual blurring\"]))\n",
        "itching=list(enc.fit_transform(data[\"Itching\"]))\n",
        "irritability=list(enc.fit_transform(data[\"Irritability\"]))\n",
        "dlyd_hlng=list(enc.fit_transform(data[\"delayed healing\"]))\n",
        "partial_paresis=list(enc.fit_transform(data[\"partial paresis\"]))\n",
        "msl_stfns=list(enc.fit_transform(data[\"muscle stiffness\"]))\n",
        "alopecia=list(enc.fit_transform(data[\"Alopecia\"]))\n",
        "obesity=list(enc.fit_transform(data[\"Obesity\"]))\n",
        "cls=list(enc.fit_transform(data[\"class\"]))"
      ],
      "metadata": {
        "id": "2o15j7wvBrgJ"
      },
      "id": "2o15j7wvBrgJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split to Test and Training Sets\n"
      ],
      "metadata": {
        "id": "Kp3iXPhCDET_"
      },
      "id": "Kp3iXPhCDET_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d2810b5",
        "outputId": "d5de9c00-bf0e-4e13-8dc2-178b6833aadd"
      },
      "source": [
        "x=np.array(list(zip(age,polyuria,polydipsia,sdn_wt_ls,weakness,\n",
        "                    polyphagia,gntl_trsh,visual_blur,itching,irritability,\n",
        "                    dlyd_hlng,partial_paresis,msl_stfns,alopecia,obesity)))\n",
        "y=np.array(list(cls))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)"
      ],
      "id": "2d2810b5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (416, 15)\n",
            "Testing set shape: (104, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23fbabd7"
      },
      "source": [
        "### Step 4: Model Selection and Training\n",
        "\n",
        "In this step, we will select appropriate machine learning models for binary classification and train them on the training data. We will choose models commonly used for such tasks."
      ],
      "id": "23fbabd7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae264ce6",
        "outputId": "579d46c4-45bb-4d41-db0d-1ef438544222"
      },
      "source": [
        "# Initialize the models\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "gb_clf = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Train the models\n",
        "print(\"Training Logistic Regression...\")\n",
        "# Now that 'Gender' is numerical, training should proceed without the ValueError\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training Random Forest Classifier...\")\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training Gradient Boosting Classifier...\")\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Models training complete.\")"
      ],
      "id": "ae264ce6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression...\n",
            "Training Random Forest Classifier...\n",
            "Training Gradient Boosting Classifier...\n",
            "Models training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81ddc607"
      },
      "source": [
        "### Step 5: Model Evaluation\n",
        "\n",
        "In this step, we will evaluate the performance of the trained models using relevant metrics to understand how well they generalize to unseen data."
      ],
      "id": "81ddc607"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1bac765",
        "outputId": "a86681a3-0374-4755-d7c1-c7d88109e159"
      },
      "source": [
        "# Evaluate Logistic Regression\n",
        "print(\"Logistic Regression Evaluation:\")\n",
        "y_pred_lr = log_reg.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, log_reg.predict_proba(X_test)[:, 1]))\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Evaluate Random Forest Classifier\n",
        "print(\"Random Forest Classifier Evaluation:\")\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, rf_clf.predict_proba(X_test)[:, 1]))\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Evaluate Gradient Boosting Classifier\n",
        "print(\"Gradient Boosting Classifier Evaluation:\")\n",
        "y_pred_gb = gb_clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_gb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb))\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, gb_clf.predict_proba(X_test)[:, 1]))\n",
        "print(\"-\" * 30)"
      ],
      "id": "b1bac765",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92        40\n",
            "           1       1.00      0.89      0.94        64\n",
            "\n",
            "    accuracy                           0.93       104\n",
            "   macro avg       0.93      0.95      0.93       104\n",
            "weighted avg       0.94      0.93      0.93       104\n",
            "\n",
            "Confusion Matrix:\n",
            " [[40  0]\n",
            " [ 7 57]]\n",
            "ROC AUC Score: 0.98984375\n",
            "------------------------------\n",
            "Random Forest Classifier Evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        40\n",
            "           1       1.00      1.00      1.00        64\n",
            "\n",
            "    accuracy                           1.00       104\n",
            "   macro avg       1.00      1.00      1.00       104\n",
            "weighted avg       1.00      1.00      1.00       104\n",
            "\n",
            "Confusion Matrix:\n",
            " [[40  0]\n",
            " [ 0 64]]\n",
            "ROC AUC Score: 1.0\n",
            "------------------------------\n",
            "Gradient Boosting Classifier Evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        40\n",
            "           1       0.98      0.98      0.98        64\n",
            "\n",
            "    accuracy                           0.98       104\n",
            "   macro avg       0.98      0.98      0.98       104\n",
            "weighted avg       0.98      0.98      0.98       104\n",
            "\n",
            "Confusion Matrix:\n",
            " [[39  1]\n",
            " [ 1 63]]\n",
            "ROC AUC Score: 0.998828125\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6c0180f"
      },
      "source": [
        "### Step 6: Hyperparameter Tuning\n",
        "\n",
        "In this step, we will tune the hyperparameters of the selected models to optimize their performance. We will use techniques like GridSearchCV or RandomizedSearchCV."
      ],
      "id": "b6c0180f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c2db5cb",
        "outputId": "e5e17e19-eb5f-4741-fca8-7196342b8053"
      },
      "source": [
        "# Hyperparameter tuning for Random Forest Classifier as an example\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(estimator=rf_clf, param_grid=param_grid_rf, cv=5, scoring='roc_auc', n_jobs=-1)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters for Random Forest:\", grid_search_rf.best_params_)\n",
        "print(\"Best ROC AUC score for Random Forest:\", grid_search_rf.best_score_)\n",
        "\n",
        "# You would repeat this process for other models (Logistic Regression, Gradient Boosting)"
      ],
      "id": "4c2db5cb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best ROC AUC score for Random Forest: 0.9964507918552036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd11477e"
      },
      "source": [
        "### Step 7: Final Model Selection and Interpretation\n",
        "\n",
        "In this step, we will select the best-performing model based on the evaluation metrics and the project's goals (considering the impact of false positives and false negatives for a medical application). We will then interpret the chosen model to understand which features are most important for prediction."
      ],
      "id": "bd11477e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d926d573"
      },
      "source": [
        "Based on the evaluation metrics from Step 5 and the hyperparameter tuning results from Step 6, we can compare the performance of the Logistic Regression, Random Forest, and Gradient Boosting models.\n",
        "\n",
        "**Comparison of Models:**\n",
        "\n",
        "*   **Logistic Regression:** [Discuss performance based on classification report, confusion matrix, and ROC AUC from Step 5]\n",
        "*   **Random Forest:** [Discuss performance based on classification report, confusion matrix, and ROC AUC from Step 5, and the impact of hyperparameter tuning from Step 6]\n",
        "*   **Gradient Boosting:** [Discuss performance based on classification report, confusion matrix, and ROC AUC from Step 5]\n",
        "\n",
        "**Final Model Selection:**\n",
        "\n",
        "Considering the project's goal of early diabetes detection for a potential Android application, it's important to balance precision and recall. [Choose the best model based on the metrics and justify your choice, e.g., \"The Random Forest model with tuned hyperparameters appears to offer the best balance of metrics for this application.\"]\n",
        "\n",
        "**Model Interpretation (Example for Random Forest - requires fitting the best model):**\n",
        "\n",
        "To understand which features are most important for the chosen model (e.g., the tuned Random Forest), we can look at feature importances."
      ],
      "id": "d926d573"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ec6c82"
      },
      "source": [
        "### Step 8: Model Export and Preparation for Deployment\n",
        "\n",
        "In this step, we will save the trained and tuned final model (e.g., using `joblib` or `pickle`). We will also document the model's requirements for deployment."
      ],
      "id": "f5ec6c82"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16e02e2c",
        "outputId": "511933fa-59ac-46da-dc4c-8b4d7be3f9b6"
      },
      "source": [
        "# Assuming 'best_model' is your selected and tuned model\n",
        "# Replace 'grid_search_rf.best_estimator_' with your chosen model after tuning all models\n",
        "\n",
        "# Example: Saving the tuned Random Forest model\n",
        "best_model = grid_search_rf.best_estimator_\n",
        "joblib.dump(best_model, 'diabetes_detection_model.pkl')\n",
        "\n",
        "print(\"Trained model saved as 'diabetes_detection_model.pkl'\")"
      ],
      "id": "16e02e2c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained model saved as 'diabetes_detection_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47895721"
      },
      "source": [
        "**Documentation for Deployment:**\n",
        "\n",
        "For deployment in an Android application, it's important to document:\n",
        "\n",
        "*   **Input Features:** The exact names and order of the features the model expects.\n",
        "*   **Data Types:** The data types of each input feature.\n",
        "*   **Preprocessing Steps:** Any preprocessing steps applied to the data before training (e.g., scaling, imputation, encoding) that need to be applied to new data before making predictions.\n",
        "*   **Model Output:** What the model's output represents (e.g., probability of having diabetes, the predicted class)."
      ],
      "id": "47895721"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c3eb1fb"
      },
      "source": [
        "### Step 9: Documentation and Presentation\n",
        "\n"
      ],
      "id": "8c3eb1fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "412680ad"
      },
      "source": [
        "### Summary of EDA Findings\n",
        "\n",
        "Based on the exploratory data analysis:\n",
        "\n",
        "*   **Dataset Structure:** The dataset contains 768 entries and 9 columns. All features appear to be numerical.\n",
        "*   **Missing Values:** There are no missing values in the dataset, as indicated by `df.isnull().sum()` showing 0 for all columns.\n",
        "*   **Target Variable Distribution:** The distribution of the target variable ('class') shows that there are 500 non-diabetic cases (class 0) and 268 diabetic cases (class 1). This indicates a slight class imbalance.\n",
        "*   **Descriptive Statistics:** The descriptive statistics (`df.describe()`) provide insights into the central tendency, spread, and range of each numerical feature. For example, the average Glucose level is around 120.89, while the average BMI is about 31.99.\n",
        "*   **Feature Distributions:** [Based on the countplot of the target variable, we can see the class distribution.] Visualizations of individual feature distributions (histograms, box plots - *These were mentioned in the plan but no code was generated for them yet. You may want to add code for these visualizations to provide more detailed insights here.*) would provide further details on the spread and common values for each feature.\n",
        "*   **Relationships with Target Variable:** [Based on correlation analysis or visualizations (which are part of the EDA plan but code hasn't been generated yet. You may want to add code for these visualizations and analysis to provide specific insights here.*)] Further analysis is needed to understand the relationship between individual features and the target variable."
      ],
      "id": "412680ad"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}